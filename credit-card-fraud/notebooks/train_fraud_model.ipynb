{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection Model Training\n",
    "\n",
    "This notebook demonstrates the training process for a credit card fraud detection model using imbalanced datasets, SMOTE, and various ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Fraud Data\n",
    "\n",
    "Since we don't have real credit card data, we'll generate synthetic data that mimics real fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fraud_data(n_samples=50000):\n",
    "    \"\"\"Generate synthetic credit card fraud data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate base features\n",
    "    data = {\n",
    "        'transaction_amount': np.random.lognormal(4, 1.5, n_samples),\n",
    "        'merchant_category': np.random.choice(['grocery', 'gas', 'restaurant', 'retail', 'online', 'atm'], n_samples),\n",
    "        'card_type': np.random.choice(['credit', 'debit'], n_samples, p=[0.7, 0.3]),\n",
    "        'transaction_type': np.random.choice(['purchase', 'withdrawal', 'transfer'], n_samples, p=[0.8, 0.15, 0.05]),\n",
    "        'hour_of_day': np.random.randint(0, 24, n_samples),\n",
    "        'day_of_week': np.random.randint(0, 7, n_samples),\n",
    "        'customer_age': np.random.randint(18, 80, n_samples),\n",
    "        'account_balance': np.random.normal(5000, 2000, n_samples),\n",
    "        'previous_transaction_amount': np.random.lognormal(4, 1.2, n_samples),\n",
    "        'transaction_frequency_24h': np.random.poisson(3, n_samples)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create fraud labels (imbalanced: ~1% fraud)\n",
    "    fraud_conditions = (\n",
    "        (df['transaction_amount'] > df['transaction_amount'].quantile(0.95)) |\n",
    "        (df['hour_of_day'].isin([2, 3, 4])) & (df['transaction_amount'] > 1000) |\n",
    "        (df['transaction_frequency_24h'] > 10) & (df['transaction_amount'] > 500) |\n",
    "        (df['transaction_amount'] > df['account_balance'] * 0.8) |\n",
    "        (np.random.random(n_samples) < 0.005)  # Random fraud\n",
    "    )\n",
    "    \n",
    "    df['is_fraud'] = fraud_conditions.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_fraud_data(50000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean():.3f}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['is_fraud'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let's explore the data to understand fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nNumerical columns summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fraud patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Transaction amount distribution\n",
    "axes[0, 0].hist(df[df['is_fraud'] == 0]['transaction_amount'], bins=50, alpha=0.7, label='Normal', density=True)\n",
    "axes[0, 0].hist(df[df['is_fraud'] == 1]['transaction_amount'], bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "axes[0, 0].set_xlabel('Transaction Amount')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Transaction Amount Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Hour of day patterns\n",
    "fraud_by_hour = df.groupby('hour_of_day')['is_fraud'].agg(['count', 'sum']).reset_index()\n",
    "fraud_by_hour['fraud_rate'] = fraud_by_hour['sum'] / fraud_by_hour['count']\n",
    "axes[0, 1].bar(fraud_by_hour['hour_of_day'], fraud_by_hour['fraud_rate'])\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Fraud Rate')\n",
    "axes[0, 1].set_title('Fraud Rate by Hour of Day')\n",
    "\n",
    "# Merchant category patterns\n",
    "fraud_by_merchant = df.groupby('merchant_category')['is_fraud'].agg(['count', 'sum']).reset_index()\n",
    "fraud_by_merchant['fraud_rate'] = fraud_by_merchant['sum'] / fraud_by_merchant['count']\n",
    "axes[1, 0].bar(fraud_by_merchant['merchant_category'], fraud_by_merchant['fraud_rate'])\n",
    "axes[1, 0].set_xlabel('Merchant Category')\n",
    "axes[1, 0].set_ylabel('Fraud Rate')\n",
    "axes[1, 0].set_title('Fraud Rate by Merchant Category')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Transaction frequency patterns\n",
    "fraud_by_freq = df.groupby('transaction_frequency_24h')['is_fraud'].agg(['count', 'sum']).reset_index()\n",
    "fraud_by_freq['fraud_rate'] = fraud_by_freq['sum'] / fraud_by_freq['count']\n",
    "axes[1, 1].bar(fraud_by_freq['transaction_frequency_24h'], fraud_by_freq['fraud_rate'])\n",
    "axes[1, 1].set_xlabel('Transaction Frequency (24h)')\n",
    "axes[1, 1].set_ylabel('Fraud Rate')\n",
    "axes[1, 1].set_title('Fraud Rate by Transaction Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data for modeling\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = ['merchant_category', 'card_type', 'transaction_type']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Create additional features\n",
    "    df_processed['amount_to_balance_ratio'] = df_processed['transaction_amount'] / (df_processed['account_balance'] + 1)\n",
    "    df_processed['amount_change_ratio'] = df_processed['transaction_amount'] / (df_processed['previous_transaction_amount'] + 1)\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        'transaction_amount', 'merchant_category', 'card_type', 'transaction_type',\n",
    "        'hour_of_day', 'day_of_week', 'customer_age', 'account_balance',\n",
    "        'previous_transaction_amount', 'transaction_frequency_24h',\n",
    "        'amount_to_balance_ratio', 'amount_change_ratio'\n",
    "    ]\n",
    "    \n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed['is_fraud']\n",
    "    \n",
    "    return X, y, label_encoders\n",
    "\n",
    "# Preprocess data\n",
    "X, y, encoders = preprocess_data(df)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance with SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic examples of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set: {y_train.value_counts()}\")\n",
    "print(f\"Balanced training set: {pd.Series(y_train_balanced).value_counts()}\")\n",
    "print(f\"Test set: {y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Models\n",
    "\n",
    "Train Random Forest and Logistic Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train models\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    trained_models[name] = model\n",
    "    print(f\"{name} training completed!\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate model performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'auc_score': auc_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['auc_score']:.3f})\", linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Fraud Detection Models')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for Fraud Detection')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Best Model\n",
    "\n",
    "Save the trained model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Select best model (Random Forest based on AUC)\n",
    "best_model = trained_models['Random Forest']\n",
    "\n",
    "# Save model components\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': encoders,\n",
    "    'feature_names': list(X.columns),\n",
    "    'auc_score': results['Random Forest']['auc_score']\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, '../fraud_detection_model_rf.pkl')\n",
    "print(\"Model saved successfully!\")\n",
    "print(f\"Model AUC-ROC Score: {results['Random Forest']['auc_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Test the model with sample transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model_data = joblib.load('../fraud_detection_model_rf.pkl')\n",
    "loaded_model = loaded_model_data['model']\n",
    "loaded_scaler = loaded_model_data['scaler']\n",
    "loaded_encoders = loaded_model_data['label_encoders']\n",
    "\n",
    "# Test with sample transactions\n",
    "test_transactions = [\n",
    "    {  # Normal transaction\n",
    "        'transaction_amount': 45.50,\n",
    "        'merchant_category': 'grocery',\n",
    "        'card_type': 'credit',\n",
    "        'transaction_type': 'purchase',\n",
    "        'hour_of_day': 14,\n",
    "        'day_of_week': 2,\n",
    "        'customer_age': 35,\n",
    "        'account_balance': 2500,\n",
    "        'previous_transaction_amount': 32.20,\n",
    "        'transaction_frequency_24h': 2\n",
    "    },\n",
    "    {  # Suspicious transaction\n",
    "        'transaction_amount': 2500,\n",
    "        'merchant_category': 'online',\n",
    "        'card_type': 'credit',\n",
    "        'transaction_type': 'purchase',\n",
    "        'hour_of_day': 3,\n",
    "        'day_of_week': 1,\n",
    "        'customer_age': 28,\n",
    "        'account_balance': 3000,\n",
    "        'previous_transaction_amount': 45.30,\n",
    "        'transaction_frequency_24h': 8\n",
    "    }\n",
    "]\n",
    "\n",
    "def predict_fraud(transaction, model, scaler, encoders):\n",
    "    \"\"\"Make fraud prediction for a single transaction\"\"\"\n",
    "    df = pd.DataFrame([transaction])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = ['merchant_category', 'card_type', 'transaction_type']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = encoders[col].transform(df[col])\n",
    "    \n",
    "    # Create additional features\n",
    "    df['amount_to_balance_ratio'] = df['transaction_amount'] / (df['account_balance'] + 1)\n",
    "    df['amount_change_ratio'] = df['transaction_amount'] / (df['previous_transaction_amount'] + 1)\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        'transaction_amount', 'merchant_category', 'card_type', 'transaction_type',\n",
    "        'hour_of_day', 'day_of_week', 'customer_age', 'account_balance',\n",
    "        'previous_transaction_amount', 'transaction_frequency_24h',\n",
    "        'amount_to_balance_ratio', 'amount_change_ratio'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Make prediction\n",
    "    fraud_probability = loaded_model.predict_proba(X_scaled)[:, 1][0]\n",
    "    is_fraud = fraud_probability > 0.5\n",
    "    \n",
    "    return {\n",
    "        'fraud_probability': fraud_probability,\n",
    "        'is_fraud': bool(is_fraud)\n",
    "    }\n",
    "\n",
    "# Test predictions\n",
    "for i, transaction in enumerate(test_transactions):\n",
    "    result = predict_fraud(transaction, loaded_model, loaded_scaler, loaded_encoders)\n",
    "    print(f\"\\nTransaction {i+1}:\")\n",
    "    print(f\"Amount: ${transaction['transaction_amount']:.2f}\")\n",
    "    print(f\"Fraud Probability: {result['fraud_probability']:.3f}\")\n",
    "    print(f\"Is Fraud: {result['is_fraud']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Generation of synthetic credit card transaction data with fraud patterns\n",
    "2. Exploratory data analysis to understand fraud characteristics\n",
    "3. Data preprocessing and feature engineering\n",
    "4. Handling class imbalance using SMOTE\n",
    "5. Training Random Forest and Logistic Regression models\n",
    "6. Model evaluation with ROC curves and classification metrics\n",
    "7. Feature importance analysis\n",
    "8. Model saving and testing\n",
    "\n",
    "The trained model can now be deployed as an API for real-time fraud detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}