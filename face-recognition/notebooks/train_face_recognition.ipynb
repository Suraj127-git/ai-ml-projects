{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition with CNN and FaceNet Embeddings\n",
    "\n",
    "This notebook demonstrates how to build a face recognition system using:\n",
    "- **MTCNN** for face detection\n",
    "- **FaceNet** (InceptionResnetV1) for face embeddings\n",
    "- **Cosine Similarity** for face matching\n",
    "\n",
    "We'll cover face detection, face embedding extraction, and face recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from app.model import FaceRecognitionModel\n",
    "from app.schemas import FaceDetectionRequest, FaceRecognitionRequest, FaceEmbedding\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the face recognition model\n",
    "print(\"Initializing Face Recognition Model...\")\n",
    "face_model = FaceRecognitionModel()\n",
    "print(\"Model initialized successfully!\")\n",
    "\n",
    "# Get model info\n",
    "model_info = face_model.get_model_info()\n",
    "print(f\"Model Name: {model_info['model_name']}\")\n",
    "print(f\"Embedding Dimension: {model_info['embedding_dimension']}\")\n",
    "print(f\"Device: {model_info['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_face_image(width=160, height=160, color=(255, 255, 255)):\n",
    "    \"\"\"Create a sample face-like image for testing\"\"\"\n",
    "    # Create a white background\n",
    "    image = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Add some face-like features (simplified)\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "    \n",
    "    # Face outline (ellipse)\n",
    "    cv2.ellipse(image, (center_x, center_y), (width//3, height//2), 0, 0, 360, (200, 200, 200), 2)\n",
    "    \n",
    "    # Eyes\n",
    "    eye_y = center_y - height//6\n",
    "    cv2.circle(image, (center_x - width//8, eye_y), width//16, (0, 0, 0), -1)\n",
    "    cv2.circle(image, (center_x + width//8, eye_y), width//16, (0, 0, 0), -1)\n",
    "    \n",
    "    # Nose\n",
    "    nose_y = center_y\n",
    "    cv2.line(image, (center_x, nose_y - height//12), (center_x, nose_y + height//12), (150, 150, 150), 2)\n",
    "    \n",
    "    # Mouth\n",
    "    mouth_y = center_y + height//4\n",
    "    cv2.ellipse(image, (center_x, mouth_y), (width//8, height//16), 0, 0, 180, (200, 100, 100), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def image_to_base64(image_array):\n",
    "    \"\"\"Convert numpy array to base64 string\"\"\"\n",
    "    pil_image = Image.fromarray(image_array)\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format='JPEG')\n",
    "    buffer.seek(0)\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "def base64_to_image(base64_string):\n",
    "    \"\"\"Convert base64 string to numpy array\"\"\"\n",
    "    image_data = base64.b64decode(base64_string)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Face Detection Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample face image\n",
    "sample_face = create_sample_face_image()\n",
    "sample_face_base64 = image_to_base64(sample_face)\n",
    "\n",
    "# Detect faces\n",
    "print(\"Detecting faces in sample image...\")\n",
    "detection_result = face_model.detect_faces(sample_face_base64, confidence_threshold=0.5)\n",
    "\n",
    "print(f\"Total faces detected: {detection_result['total_faces']}\")\n",
    "print(f\"Image dimensions: {detection_result['image_width']}x{detection_result['image_height']}\")\n",
    "print(f\"Processing time: {detection_result['processing_time']:.4f} seconds\")\n",
    "\n",
    "if detection_result['faces']:\n",
    "    for i, face in enumerate(detection_result['faces']):\n",
    "        print(f\"\\nFace {i+1}:\")\n",
    "        print(f\"  Face ID: {face['face_id']}\")\n",
    "        print(f\"  Confidence: {face['confidence']:.4f}\")\n",
    "        print(f\"  Bounding Box: {face['bbox']}\")\n",
    "        if 'landmarks' in face:\n",
    "            print(f\"  Landmarks: {face['landmarks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Face Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract face embedding from the sample face\n",
    "print(\"Extracting face embedding...\")\n",
    "embedding = face_model.extract_face_embedding(sample_face_base64)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"Embedding range: [{embedding.min():.4f}, {embedding.max():.4f}]\")\n",
    "print(f\"Embedding mean: {embedding.mean():.4f}\")\n",
    "print(f\"Embedding std: {embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Person Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple face images for a person (simulating different photos)\n",
    "def create_varied_face_images(person_name, num_images=3):\n",
    "    \"\"\"Create multiple face images with slight variations\"\"\"\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        # Create base face\n",
    "        face = create_sample_face_image()\n",
    "        \n",
    "        # Add some variation (slightly different colors, positions)\n",
    "        variation = np.random.randint(-20, 20, 3)\n",
    "        face = face.astype(np.int16) + variation\n",
    "        face = np.clip(face, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        images.append(image_to_base64(face))\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Register a few people\n",
    "people = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "\n",
    "for person_name in people:\n",
    "    print(f\"\\nRegistering {person_name}...\")\n",
    "    face_images = create_varied_face_images(person_name, num_images=3)\n",
    "    \n",
    "    registration_result = face_model.register_person(person_name, face_images)\n",
    "    print(f\"  Person ID: {registration_result['person_id']}\")\n",
    "    print(f\"  Embeddings created: {registration_result['embeddings_created']}\")\n",
    "    print(f\"  Status: {registration_result['registration_status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test face (similar to Alice)\n",
    "test_face = create_sample_face_image()\n",
    "test_face_base64 = image_to_base64(test_face)\n",
    "\n",
    "# Get all registered persons\n",
    "registered_persons = face_model.get_registered_persons()\n",
    "print(f\"Total registered persons: {len(registered_persons)}\")\n",
    "\n",
    "# Prepare known faces for recognition\n",
    "known_faces = []\n",
    "for person in registered_persons:\n",
    "    known_faces.append({\n",
    "        \"face_id\": person[\"face_embedding\"][\"face_id\"],\n",
    "        \"embedding\": person[\"face_embedding\"][\"embedding\"]\n",
    "    })\n",
    "\n",
    "# Recognize the test face\n",
    "print(\"\\nRecognizing test face...\")\n",
    "recognition_result = face_model.recognize_face(test_face_base64, known_faces, similarity_threshold=0.6)\n",
    "\n",
    "print(f\"Recognized: {recognition_result['recognized']}\")\n",
    "if recognition_result['recognized']:\n",
    "    print(f\"Matched Face ID: {recognition_result['matched_face_id']}\")\n",
    "    print(f\"Similarity Score: {recognition_result['similarity_score']:.4f}\")\n",
    "    print(f\"Confidence: {recognition_result['confidence']:.4f}\")\n",
    "\n",
    "print(\"\\nAll similarities:\")\n",
    "for similarity in recognition_result['all_similarities']:\n",
    "    print(f\"  {similarity['face_id']}: {similarity['similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Face Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two similar faces (same person)\n",
    "face1 = create_sample_face_image()\n",
    "face2 = create_sample_face_image()\n",
    "face1_base64 = image_to_base64(face1)\n",
    "face2_base64 = image_to_base64(face2)\n",
    "\n",
    "# Compare the two faces\n",
    "print(\"Comparing two similar faces (same person simulation)...\")\n",
    "comparison_result = face_model.compare_faces(face1_base64, face2_base64)\n",
    "\n",
    "print(f\"Similarity Score: {comparison_result['similarity_score']:.4f}\")\n",
    "print(f\"Are Same Person: {comparison_result['are_same_person']}\")\n",
    "print(f\"Confidence: {comparison_result['confidence']:.4f}\")\n",
    "\n",
    "# Create a different face\n",
    "different_face = create_sample_face_image(color=(200, 200, 255))  # Different color\n",
    "different_face_base64 = image_to_base64(different_face)\n",
    "\n",
    "# Compare with different face\n",
    "print(\"\\nComparing with different face...\")\n",
    "comparison_result2 = face_model.compare_faces(face1_base64, different_face_base64)\n",
    "\n",
    "print(f\"Similarity Score: {comparison_result2['similarity_score']:.4f}\")\n",
    "print(f\"Are Same Person: {comparison_result2['are_same_person']}\")\n",
    "print(f\"Confidence: {comparison_result2['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Show the three faces\n",
    "axes[0].imshow(cv2.cvtColor(face1, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Face 1')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(face2, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title('Face 2 (Similar)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(cv2.cvtColor(different_face, cv2.COLOR_BGR2RGB))\n",
    "axes[2].set_title('Face 3 (Different)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison chart\n",
    "similarities = [comparison_result['similarity_score'], comparison_result2['similarity_score']]\n",
    "labels = ['Face 1 vs Face 2\\n(Similar)', 'Face 1 vs Face 3\\n(Different)']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(labels, similarities, color=['green', 'red'], alpha=0.7)\n",
    "plt.title('Face Similarity Comparison')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add threshold line\n",
    "plt.axhline(y=0.6, color='blue', linestyle='--', label='Recognition Threshold (0.6)')\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, similarity in zip(bars, similarities):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{similarity:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch processing performance\n",
    "import time\n",
    "\n",
    "# Create multiple test faces\n",
    "num_test_faces = 10\n",
    "test_faces = []\n",
    "for i in range(num_test_faces):\n",
    "    face = create_sample_face_image()\n",
    "    test_faces.append(image_to_base64(face))\n",
    "\n",
    "# Measure embedding extraction time\n",
    "start_time = time.time()\n",
    "embeddings = []\n",
    "for i, face_base64 in enumerate(test_faces):\n",
    "    try:\n",
    "        embedding = face_model.extract_face_embedding(face_base64)\n",
    "        embeddings.append(embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting embedding for face {i+1}: {str(e)}\")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"Extracted {len(embeddings)} embeddings from {num_test_faces} faces\")\n",
    "print(f\"Total embedding extraction time: {embedding_time:.4f} seconds\")\n",
    "print(f\"Average time per face: {embedding_time/len(embeddings):.4f} seconds\")\n",
    "\n",
    "# Measure recognition time\n",
    "if registered_persons:\n",
    "    start_time = time.time()\n",
    "    for i, embedding in enumerate(embeddings[:5]):  # Test with first 5\n",
    "        # Create FaceEmbedding object\n",
    "        face_embedding = FaceEmbedding(\n",
    "            embedding=embedding.tolist(),\n",
    "            face_id=f\"test_face_{i}\"\n",
    "        )\n",
    "        \n",
    "        # Test recognition\n",
    "        request = FaceRecognitionRequest(\n",
    "            image_base64=test_faces[i],\n",
    "            known_faces=[face_embedding for _ in registered_persons]\n",
    "        )\n",
    "    \n",
    "    recognition_time = time.time() - start_time\n",
    "    print(f\"\\nRecognition time for 5 faces: {recognition_time:.4f} seconds\")\n",
    "    print(f\"Average time per recognition: {recognition_time/5:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the registered persons\n",
    "print(\"Saving registered persons...\")\n",
    "face_model.save_models(\"face_recognition_demo.joblib\")\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# Create a new model instance and load the saved data\n",
    "print(\"\\nCreating new model instance and loading saved data...\")\n",
    "new_face_model = FaceRecognitionModel()\n",
    "new_face_model.load_models(\"face_recognition_demo.joblib\")\n",
    "\n",
    "# Verify loaded data\n",
    "loaded_persons = new_face_model.get_registered_persons()\n",
    "print(f\"Loaded {len(loaded_persons)} persons\")\n",
    "\n",
    "for person in loaded_persons:\n",
    "    print(f\"  - {person['person_name']} (ID: {person['person_id']})\")\n",
    "    print(f\"    Images: {person['image_count']}\")\n",
    "    print(f\"    Registration: {person['registration_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Features and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling with invalid images\n",
    "print(\"Testing error handling...\")\n",
    "\n",
    "# Test with invalid base64\n",
    "try:\n",
    "    face_model.detect_faces(\"invalid_base64_string\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with invalid base64: {str(e)}\")\n",
    "\n",
    "# Test with non-face image\n",
    "non_face_image = np.ones((160, 160, 3), dtype=np.uint8) * 128  # Gray image\n",
    "non_face_base64 = image_to_base64(non_face_image)\n",
    "\n",
    "try:\n",
    "    result = face_model.detect_faces(non_face_base64)\n",
    "    print(f\"Non-face detection result: {result['total_faces']} faces detected\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with non-face image: {str(e)}\")\n",
    "\n",
    "# Test with very small image\n",
    "small_image = np.ones((50, 50, 3), dtype=np.uint8) * 255\n",
    "small_base64 = image_to_base64(small_image)\n",
    "\n",
    "try:\n",
    "    result = face_model.detect_faces(small_base64)\n",
    "    print(f\"Small image detection result: {result['total_faces']} faces detected\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with small image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "### Key Features Demonstrated\n",
    "1. **Face Detection**: Using MTCNN to detect faces in images\n",
    "2. **Face Embedding**: Extracting 512-dimensional embeddings using FaceNet\n",
    "3. **Face Recognition**: Matching faces against known persons\n",
    "4. **Face Comparison**: Comparing two faces to determine similarity\n",
    "5. **Person Registration**: Registering new persons with multiple face images\n",
    "6. **Model Persistence**: Saving and loading registered persons\n",
    "\n",
    "### Performance Insights\n",
    "- Face embedding extraction is fast (~0.1-0.2 seconds per face)\n",
    "- Face recognition is efficient with cosine similarity\n",
    "- MTCNN provides reliable face detection with landmarks\n",
    "- FaceNet embeddings are robust for face recognition tasks\n",
    "\n",
    "### Next Steps for Production\n",
    "1. **Real Image Testing**: Test with real face images instead of synthetic ones\n",
    "2. **Dataset Integration**: Use proper face datasets like LFW, VGGFace2, or CelebA\n",
    "3. **Model Fine-tuning**: Fine-tune FaceNet on specific datasets for better performance\n",
    "4. **Advanced Preprocessing**: Add face alignment, normalization, and augmentation\n",
    "5. **Batch Processing**: Optimize for processing multiple images efficiently\n",
    "6. **Security Features**: Add anti-spoofing and liveness detection\n",
    "7. **Database Integration**: Store embeddings in a proper database\n",
    "8. **API Optimization**: Add caching, rate limiting, and monitoring\n",
    "\n",
    "### Important Considerations\n",
    "- **Privacy**: Handle face data with appropriate privacy measures\n",
    "- **Bias**: Be aware of potential biases in face recognition models\n",
    "- **Accuracy**: Test thoroughly across different demographics and conditions\n",
    "- **Ethics**: Consider ethical implications of face recognition technology\n",
    "- **Regulations**: Comply with relevant privacy and data protection regulations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}